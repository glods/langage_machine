{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Le Langage des Machines - Acte 0\n",
    "## Le vocabulaire du NLP\n",
    "\n",
    "**Auteur :** Glorie M. WOWO | Data Scientist\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectif de ce notebook\n",
    "\n",
    "Avant de plonger dans les techniques de NLP, nous devons ma√Ætriser le **vocabulaire** de ce domaine.\n",
    "\n",
    "| Terme | Description |\n",
    "|-------|-------------|\n",
    "| Token | Unit√© √©l√©mentaire extraite d'un texte |\n",
    "| Tokenisation | Processus de d√©coupage en tokens |\n",
    "| Corpus | Collection de textes |\n",
    "| Vocabulaire | Ensemble des tokens uniques |\n",
    "| Document | Une unit√© de texte dans le corpus |\n",
    "| Pr√©traitement | Nettoyage et normalisation |\n",
    "| Embedding | Repr√©sentation vectorielle du sens |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Le texte brut : notre mati√®re premi√®re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un texte du programme SUCCEED\n",
    "texte = \"Dans le programme SUCCEED, les enfants apprennent l'informatique.\"\n",
    "\n",
    "print(\"Pour nous : une phrase qui √©voque des jeunes qui d√©couvrent le code.\")\n",
    "print(f\"Pour la machine : une cha√Æne de {len(texte)} caract√®res.\")\n",
    "print(f\"\\nTexte : '{texte}'\")\n",
    "print(f\"\\nCe que la machine 'voit' :\")\n",
    "print(list(texte))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token : l'unit√© √©l√©mentaire\n",
    "\n",
    "Un **token** peut √™tre un mot, un sous-mot, ou un caract√®re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIFF√âRENTS TYPES DE TOKENS\n",
      "========================================\n",
      "Texte : 'Les enfants codent en Python.'\n",
      "\n",
      "1. Par MOTS : ['Les', 'enfants', 'codent', 'en', 'Python.']\n",
      "\n",
      "2. Par CARACT√àRES : ['L', 'e', 's', ' ', 'e', 'n', 'f', 'a', 'n', 't', 's', ' ', 'c', 'o', 'd']...\n",
      "\n",
      "3. Par SOUS-MOTS : ['Les', ' enfants', ' cod', 'ent', ' en', ' Python', '.']\n",
      "   -> C'est ce que font GPT, Claude, LLaMA...\n"
     ]
    }
   ],
   "source": [
    "texte = \"Les enfants codent en Python.\"\n",
    "\n",
    "print(\"DIFF√âRENTS TYPES DE TOKENS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Texte : '{texte}'\")\n",
    "\n",
    "# Tokens par mots\n",
    "tokens_mots = texte.split()\n",
    "print(f\"\\n1. Par MOTS : {tokens_mots}\")\n",
    "\n",
    "# Tokens par caract√®res\n",
    "tokens_chars = list(texte)\n",
    "print(f\"\\n2. Par CARACT√àRES : {tokens_chars[:15]}...\")\n",
    "\n",
    "# Tokens par sous-mots (simulation)\n",
    "tokens_sousmots = [\"Les\", \" enfants\", \" cod\", \"ent\", \" en\", \" Python\", \".\"]\n",
    "print(f\"\\n3. Par SOUS-MOTS : {tokens_sousmots}\")\n",
    "print(\"   -> C'est ce que font GPT, Claude, LLaMA...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ‚úÇÔ∏è Tokenisation : le d√©coupage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSUS DE TOKENISATION\n",
      "========================================\n",
      "ENTR√âE : 'Dans le programme SUCCEED, les enfants apprennent l'informatique.'\n",
      "\n",
      "SORTIE : ['dans', 'le', 'programme', 'succeed,', 'les', 'enfants', 'apprennent', \"l'informatique.\"]\n",
      "-> 8 tokens extraits\n"
     ]
    }
   ],
   "source": [
    "def tokenize_simple(texte):\n",
    "    \"\"\"Tokenisation simple par mots.\"\"\"\n",
    "    texte = texte.lower()\n",
    "    tokens = texte.split()\n",
    "    return tokens\n",
    "\n",
    "texte = \"Dans le programme SUCCEED, les enfants apprennent l'informatique.\"\n",
    "\n",
    "print(\"PROCESSUS DE TOKENISATION\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"ENTR√âE : '{texte}'\")\n",
    "\n",
    "tokens = tokenize_simple(texte)\n",
    "print(f\"\\nSORTIE : {tokens}\")\n",
    "print(f\"-> {len(tokens)} tokens extraits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìö Corpus : la collection de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS DE RETOURS D'√âL√àVES SUCCEED\n",
      "========================================\n",
      "Nombre de documents : 5\n",
      "\n",
      "Document 1 : 'J'ai appris √† cr√©er mon premier site web !'\n",
      "Document 2 : 'Python est plus facile que je pensais.'\n",
      "Document 3 : 'Je veux devenir d√©veloppeuse !'\n",
      "Document 4 : 'Les formateurs expliquent tr√®s bien.'\n",
      "Document 5 : 'J'adore coder des jeux vid√©o.'\n"
     ]
    }
   ],
   "source": [
    "# Corpus de retours d'√©l√®ves SUCCEED\n",
    "corpus = [\n",
    "    \"J'ai appris √† cr√©er mon premier site web !\",\n",
    "    \"Python est plus facile que je pensais.\",\n",
    "    \"Je veux devenir d√©veloppeuse !\",\n",
    "    \"Les formateurs expliquent tr√®s bien.\",\n",
    "    \"J'adore coder des jeux vid√©o.\"\n",
    "]\n",
    "\n",
    "print(\"CORPUS DE RETOURS D'√âL√àVES SUCCEED\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Nombre de documents : {len(corpus)}\\n\")\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    print(f\"Document {i+1} : '{doc}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìñ Vocabulaire : le dictionnaire du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULAIRE DU CORPUS\n",
      "========================================\n",
      "Corpus : ['les enfants codent', 'les enfants apprennent', 'les formateurs enseignent']\n",
      "\n",
      "Vocabulaire (6 tokens uniques) :\n",
      "   ‚Ä¢ apprennent\n",
      "   ‚Ä¢ codent\n",
      "   ‚Ä¢ enfants\n",
      "   ‚Ä¢ enseignent\n",
      "   ‚Ä¢ formateurs\n",
      "   ‚Ä¢ les\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(corpus):\n",
    "    \"\"\"Construit le vocabulaire (tokens uniques) d'un corpus.\"\"\"\n",
    "    vocabulary = set()\n",
    "    for document in corpus:\n",
    "        tokens = document.lower().split()\n",
    "        vocabulary.update(tokens)\n",
    "    return vocabulary\n",
    "\n",
    "# Corpus simplifi√©\n",
    "corpus_simple = [\n",
    "    \"les enfants codent\",\n",
    "    \"les enfants apprennent\",\n",
    "    \"les formateurs enseignent\"\n",
    "]\n",
    "\n",
    "vocab = build_vocabulary(corpus_simple)\n",
    "\n",
    "print(\"VOCABULAIRE DU CORPUS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Corpus : {corpus_simple}\")\n",
    "print(f\"\\nVocabulaire ({len(vocab)} tokens uniques) :\")\n",
    "for token in sorted(vocab):\n",
    "    print(f\"   ‚Ä¢ {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULAIRE INDEX√â\n",
      "========================================\n",
      "Mapping token ‚Üí index :\n",
      "   'apprennent' -> 0\n",
      "   'codent' -> 1\n",
      "   'enfants' -> 2\n",
      "   'enseignent' -> 3\n",
      "   'formateurs' -> 4\n",
      "   'les' -> 5\n"
     ]
    }
   ],
   "source": [
    "# Vocabulaire index√© (n√©cessaire pour la vectorisation)\n",
    "vocab_to_idx = {token: idx for idx, token in enumerate(sorted(vocab))}\n",
    "\n",
    "print(\"VOCABULAIRE INDEX√â\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Mapping token ‚Üí index :\")\n",
    "for token, idx in vocab_to_idx.items():\n",
    "    print(f\"   '{token}' -> {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìÑ Document et S√©quence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE vs S√âQUENCE\n",
      "========================================\n",
      "Texte : 'les enfants codent'\n",
      "\n",
      "Comme ENSEMBLE : {'codent', 'enfants', 'les'}\n",
      "   -> L'ordre est perdu (Bag of Words)\n",
      "\n",
      "Comme S√âQUENCE : ('les', 'enfants', 'codent')\n",
      "   -> L'ordre est pr√©serv√© (N-grammes, Transformers)\n"
     ]
    }
   ],
   "source": [
    "texte = \"les enfants codent\"\n",
    "tokens = texte.split()\n",
    "\n",
    "print(\"ENSEMBLE vs S√âQUENCE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Texte : '{texte}'\")\n",
    "\n",
    "# Comme ensemble (pas d'ordre)\n",
    "ensemble = set(tokens)\n",
    "print(f\"\\nComme ENSEMBLE : {ensemble}\")\n",
    "print(\"   -> L'ordre est perdu (Bag of Words)\")\n",
    "\n",
    "# Comme s√©quence (ordre pr√©serv√©)\n",
    "sequence = tuple(tokens)\n",
    "print(f\"\\nComme S√âQUENCE : {sequence}\")\n",
    "print(\"   -> L'ordre est pr√©serv√© (N-grammes, Transformers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üßπ Pr√©traitement (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âTAPES DE PR√âTRAITEMENT\n",
      "==================================================\n",
      "Original : 'Les ENFANTS du programme SUCCEED codent!!! √Ä Douala, en 2024.'\n",
      "\n",
      "1. Lowercasing : 'les enfants du programme succeed codent!!! √† douala, en 2024.'\n",
      "2. Sans ponctuation : 'les enfants du programme succeed codent √† douala en 2024'\n",
      "3. Sans nombres : 'les enfants du programme succeed codent √† douala en '\n",
      "4. Nettoy√© : 'les enfants du programme succeed codent √† douala en'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "texte_original = \"Les ENFANTS du programme SUCCEED codent!!! √Ä Douala, en 2024.\"\n",
    "\n",
    "print(\"√âTAPES DE PR√âTRAITEMENT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original : '{texte_original}'\")\n",
    "\n",
    "# 1. Lowercasing\n",
    "texte = texte_original.lower()\n",
    "print(f\"\\n1. Lowercasing : '{texte}'\")\n",
    "\n",
    "# 2. Suppression ponctuation\n",
    "texte_no_punct = re.sub(r'[^\\w\\s]', '', texte)\n",
    "print(f\"2. Sans ponctuation : '{texte_no_punct}'\")\n",
    "\n",
    "# 3. Suppression nombres\n",
    "texte_no_numbers = re.sub(r'\\d+', '', texte_no_punct)\n",
    "print(f\"3. Sans nombres : '{texte_no_numbers}'\")\n",
    "\n",
    "# 4. Nettoyage espaces\n",
    "texte_clean = ' '.join(texte_no_numbers.split())\n",
    "print(f\"4. Nettoy√© : '{texte_clean}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPRESSION DES STOP WORDS\n",
      "========================================\n",
      "Tokens : ['les', 'enfants', 'du', 'programme', 'codent', 'dans', 'la', 'salle']\n",
      "Apr√®s filtrage : ['enfants', 'programme', 'codent', 'salle']\n",
      "‚Üí On garde l'essentiel : enfants, programme, codent, salle\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "stop_words_fr = {'le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', \n",
    "                 'et', 'en', '√†', 'au', 'dans', 'qui', 'que'}\n",
    "\n",
    "texte = \"les enfants du programme codent dans la salle\"\n",
    "tokens = texte.split()\n",
    "\n",
    "print(\"SUPPRESSION DES STOP WORDS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Tokens : {tokens}\")\n",
    "\n",
    "tokens_filtered = [t for t in tokens if t not in stop_words_fr]\n",
    "print(f\"Apr√®s filtrage : {tokens_filtered}\")\n",
    "print(\"‚Üí On garde l'essentiel : enfants, programme, codent, salle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEMMING vs LEMMATISATION\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "STEMMING (r√©duction au radical)\n",
    "  \"enseignaient\" ‚Üí \"enseign\"\n",
    "  \"enseignera\"   ‚Üí \"enseign\"\n",
    "  ‚Üí Rapide mais impr√©cis\n",
    "\n",
    "LEMMATISATION (forme de base)\n",
    "  \"enseignaient\" ‚Üí \"enseigner\"\n",
    "  \"apprennent\"   ‚Üí \"apprendre\"\n",
    "  ‚Üí Plus lent mais pr√©cis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üî¢ Vecteur et Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QU'EST-CE QU'UN VECTEUR ?\n",
      "========================================\n",
      "Vecteur : [0.2 0.8 0.1 0.5]\n",
      "Dimension : 4\n",
      "\n",
      "Exemple : repr√©senter 'les enfants codent'\n",
      "Vocabulaire : ['apprennent', 'codent', 'enfants', 'enseignent', 'formateurs', 'les']\n",
      "Vecteur BoW : [0, 1, 1, 0, 0, 1]\n",
      "              (apprennent=0, codent=1, enfants=1, enseignent=0, formateurs=0, les=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"QU'EST-CE QU'UN VECTEUR ?\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Un vecteur simple\n",
    "vecteur = np.array([0.2, 0.8, 0.1, 0.5])\n",
    "print(f\"Vecteur : {vecteur}\")\n",
    "print(f\"Dimension : {len(vecteur)}\")\n",
    "\n",
    "# Repr√©sentation d'un document\n",
    "print(f\"\\nExemple : repr√©senter 'les enfants codent'\")\n",
    "print(f\"Vocabulaire : ['apprennent', 'codent', 'enfants', 'enseignent', 'formateurs', 'les']\")\n",
    "print(f\"Vecteur BoW : [0, 1, 1, 0, 0, 1]\")\n",
    "print(f\"              (apprennent=0, codent=1, enfants=1, enseignent=0, formateurs=0, les=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDINGS (APER√áU)\n",
      "========================================\n",
      "\n",
      "Un embedding est un vecteur DENSE qui capture le SENS :\n",
      "\n",
      "  \"informatique\" ‚Üí [0.32, -0.15, 0.87, 0.12, ..., -0.43]  (300+ dimensions)\n",
      "  \"programmation\" ‚Üí [0.30, -0.12, 0.85, 0.10, ..., -0.40]  (proche!)\n",
      "  \"cuisine\"       ‚Üí [-0.54, 0.67, -0.23, 0.89, ..., 0.12]  (loin)\n",
      "\n",
      "La magie de Word2Vec :\n",
      "  \"roi\" - \"homme\" + \"femme\" ‚âà \"reine\"\n",
      "  \"Paris\" - \"France\" + \"Italie\" ‚âà \"Rome\"\n",
      "\n",
      "‚Üí Nous explorerons cela dans l'Acte d√©di√© aux Embeddings !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"EMBEDDINGS (APER√áU)\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\"\"\n",
    "Un embedding est un vecteur DENSE qui capture le SENS :\n",
    "\n",
    "  \"informatique\" ‚Üí [0.32, -0.15, 0.87, 0.12, ..., -0.43]  (300+ dimensions)\n",
    "  \"programmation\" ‚Üí [0.30, -0.12, 0.85, 0.10, ..., -0.40]  (proche!)\n",
    "  \"cuisine\"       ‚Üí [-0.54, 0.67, -0.23, 0.89, ..., 0.12]  (loin)\n",
    "\n",
    "La magie de Word2Vec :\n",
    "  \"roi\" - \"homme\" + \"femme\" ‚âà \"reine\"\n",
    "  \"Paris\" - \"France\" + \"Italie\" ‚âà \"Rome\"\n",
    "\n",
    "‚Üí Nous explorerons cela dans l'Acte d√©di√© aux Embeddings !\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üìä R√©capitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOSSAIRE NLP\n",
      "==================================================\n",
      "\n",
      "Token\n",
      "   Unit√© √©l√©mentaire (mot, sous-mot, caract√®re)\n",
      "\n",
      "Tokenisation\n",
      "   Processus de d√©coupage en tokens\n",
      "\n",
      "Tokenizer\n",
      "   L'outil qui effectue la tokenisation\n",
      "\n",
      "Corpus\n",
      "   Collection de textes pour entra√Æner un mod√®le\n",
      "\n",
      "Vocabulaire\n",
      "   Ensemble des tokens uniques du corpus\n",
      "\n",
      "Document\n",
      "   Une unit√© de texte dans le corpus\n",
      "\n",
      "S√©quence\n",
      "   Suite ordonn√©e de tokens (l'ordre compte)\n",
      "\n",
      "Pr√©traitement\n",
      "   Nettoyage et normalisation du texte\n",
      "\n",
      "Stop Words\n",
      "   Mots trop fr√©quents qu'on filtre\n",
      "\n",
      "Vecteur\n",
      "   Liste de nombres (format pour le ML)\n",
      "\n",
      "Embedding\n",
      "   Repr√©sentation vectorielle dense du sens\n"
     ]
    }
   ],
   "source": [
    "glossaire = {\n",
    "    \"Token\": \"Unit√© √©l√©mentaire (mot, sous-mot, caract√®re)\",\n",
    "    \"Tokenisation\": \"Processus de d√©coupage en tokens\",\n",
    "    \"Tokenizer\": \"L'outil qui effectue la tokenisation\",\n",
    "    \"Corpus\": \"Collection de textes pour entra√Æner un mod√®le\",\n",
    "    \"Vocabulaire\": \"Ensemble des tokens uniques du corpus\",\n",
    "    \"Document\": \"Une unit√© de texte dans le corpus\",\n",
    "    \"S√©quence\": \"Suite ordonn√©e de tokens (l'ordre compte)\",\n",
    "    \"Pr√©traitement\": \"Nettoyage et normalisation du texte\",\n",
    "    \"Stop Words\": \"Mots trop fr√©quents qu'on filtre\",\n",
    "    \"Vecteur\": \"Liste de nombres (format pour le ML)\",\n",
    "    \"Embedding\": \"Repr√©sentation vectorielle dense du sens\",\n",
    "}\n",
    "\n",
    "print(\"GLOSSAIRE NLP\")\n",
    "print(\"=\" * 50)\n",
    "for terme, definition in glossaire.items():\n",
    "    print(f\"\\n{terme}\")\n",
    "    print(f\"   {definition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Conclusion\n",
    "\n",
    "Nous avons maintenant le **vocabulaire** n√©cessaire pour comprendre les techniques de NLP.\n",
    "\n",
    "### Prochaine √©tape : Acte I ‚Äî Bag of Words\n",
    "\n",
    "Dans le prochain notebook, nous verrons comment transformer un texte en vecteur de nombres avec la technique la plus simple : **compter les mots**.\n",
    "\n",
    "---\n",
    "\n",
    "*Le Langage des Machines ‚Äî Glorie M. WOWO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
